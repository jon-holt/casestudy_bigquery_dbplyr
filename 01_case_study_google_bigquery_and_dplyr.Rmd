---
title: "Connecting to Google BigQuery with DBplyr"
author: "John R Little"
date:  "`r Sys.Date()`"
output: html_notebook
---

## Google Cloud Account & Big Query

1) Get a Google Cloud account

    - [BigQuery sandbox](https://cloud.google.com/bigquery/docs/sandbox)
    - [Google Cloud Free Program](https://cloud.google.com/free/docs/gcp-free-tier)
    - [BigQuery public datasets](https://cloud.google.com/bigquery/public-data)
    - [BigQuery](https://cloud.google.com/bigquery)
    - [GCP-BigQuery Console](https://console.cloud.google.com/bigquery)

No Credit Card?  [Use the BigQuery sandbox](https://cloud.google.com/blog/products/data-analytics/query-without-a-credit-card-introducing-bigquery-sandbox) 



The first part of this demonstration is inspired by [Kevin Wang's article on BigQuery in R](https://kevinwang09.github.io/post/bigquery-in-r/).  In this example we will query [JHU's Covid19](https://github.com/CSSEGISandData/COVID-19) public dataset. moo


## Library packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(DBI) 
library(bigrquery)
```


BigQuery refers to tables in the format `database.dataset.table`. For example, you can run this query in the BigQuery Console

```
select * from bigquery-public-data.stackoverflow.users limit 10;
```

## Establish a database connection

First create a new GCP project in the GCP.
The `dbConnect()` argument, `billing`, should have the value of a GCP **project ID**

GPC > Home > Dashboard
GPC > BigQuery

```{r}
# library(DBI)

con <- dbConnect(
  bigquery(),
  project = "bigquery-public-data",
  dataset = "covid19_jhu_csse",
  billing = "workshop-rfun-2021-spring"
)
con
```

Now, from **within the R console**, authenticate with GCP

```
bigrquery::bq_auth()
```
## Create a pointer to a Google BQ database table

Our goal is to investigate and search the JHU COVID19 _summary_ **table**.  i.e. bigquery-public-data.covid19_jhu_csse.summary


```{r}
my_db_pointer <- tbl(con, "summary")
```


Now I can use [dplyr verbs](https://dplyr.tidyverse.org/) to explore the db table.  


```{r}
# roughly:  SELECT * FROM bigquery-public-data.covid19_jhu_csse.summary limit 10;
glimpse(my_db_pointer) 
```


```{r}
# roughly:  SELECT count( * ) FROM summary
count(my_db_pointer) 
```



## SQL

Using `DBI`, you can compose SQL directly.  See also, [Writing SQL with dbplyr](https://dbplyr.tidyverse.org/articles/sql.html)

```
{r}
DBI::dbGetQuery(con, 
"SELECT *
FROM `bigquery-public-data.covid19_jhu_csse.summary`
LIMIT 10;")
```

### Examples of a more complex SQL query
```
{r}
jhu_covid19_nc_counties_10 <- dbGetQuery(con,
           "SELECT
              admin2, province_state, confirmed
            FROM
              `bigquery-public-data.covid19_jhu_csse.summary` 
            WHERE 
              country_region = 'US'
              AND date = '2020-06-30'
              AND province_state = 'North Carolina'
            LIMIT 10;"
)
jhu_covid19_nc_counties_10
```


```
{r}
jhu_covid19_nc_counties <- dbGetQuery(con,
           "SELECT
              admin2, province_state, confirmed
            FROM
              `bigquery-public-data.covid19_jhu_csse.summary` 
            WHERE 
              country_region = 'US'
              AND date = '2020-06-30'
              AND province_state = 'North Carolina'"
)
jhu_covid19_nc_counties
```

Similar to `glimpse()` but more SQLish

```{r}
bigrquery::bq_table_fields("bigquery-public-data.austin_bikeshare.bikeshare_trips")
```



## dpplyr/dbplyr approach

If you don't know SQL, or are more comfortable with `dplyr`, you can compose your queries with `dplyr` verbs.  `dbplyr` is the backend to `dplyr` and will broker the process.  In practice you do not need to call `dbplyr` and the easiest thing to do is load `library(tidyverse)` at the top of your script.

First _connect to a table_ using the `tbl()` function.  This assumes you have already used `DBI::dbConnect()` to broker the database connection -- as we did above.

```{r}
covid_data_DBplyr_style <- tbl(con, "summary") 
```



> Note: `collect()` will run a query that has been assigned to an object. 

`collect()` will activate your SQL query.  Normally, I will try to use use the `collect()` sparingly to economize my connections to the remote database, and limit the data I am requesting from the DB server.  My goal is push my data processing up to the RDBMS server as much as possible.

```
covid_data_DBplyr_style %>% 
  collect()   # this will gather all the data from the summary table
```

Above will pull the entire data table down into local RAM.  A better approach is to leverage dplyr.  Let dplyr/dbplyr translate queries into SQL and send those to the SQL engine.  This allows use to use the RDMBS for summarizing data while using local RAM and CPU for retrieving only the data you want to manipulate.


```{r}
jhu_covid19_DBP_nc_counties <- covid_data_DBplyr_style %>% 
  filter(province_state == "North Carolina", date == '2020-06-30')  %>% 
  select(province_state, country_region, date, latitude, 
         longitude, confirmed, deaths, #location_geom,
         recovered, active, fips, admin2, combined_key) 

jhu_covid19_DBP_nc_counties 
```

### show_query()

If you want to see the SQL

```{r}
jhu_covid19_DBP_nc_counties %>% 
  show_query()
```

See Also [Writing SQL with dbplyr](https://dbplyr.tidyverse.org/articles/sql.html)  
See Also dbplyr > Articles > Verb translation  
See Also dbplyr > Articles > Function translation  

### Variations

The following are some initial variations using dplyr verbs to create SQL queries, and connect with the RDMBS in a client/server fashion.

```{r}
glimpse(my_db_pointer)

my_search <- my_db_pointer %>% 
  filter(province_state == "North Carolina", date == '2020-06-30')  %>% 
  select(province_state, country_region, date, latitude, 
         longitude, confirmed, deaths, #location_geom,
         recovered, active, fips, admin2, combined_key)

my_search  

my_search %>% 
  collect()
```

### Collecting all the data into a local tibble

```{r}
jhu_covid19_durham_sinceApril <- covid_data_DBplyr_style %>% 
  filter(province_state == "North Carolina", 
         admin2 == "Durham",
         date >= '2020-04-01')  %>% 
  select(date, confirmed, deaths, recovered, active, 
         fips, admin2, combined_key) 

my_local_tbl <- jhu_covid19_durham_sinceApril %>% 
  collect()

my_local_tbl
```


```{r}
jhu_covid19_durham_sinceApril %>% 
  arrange(date) 

my_local_tbl %>% 
  arrange(date)
```

### Visualize

Creating visualizations requires 100% of the data.  the [dbplot package](https://db.rstudio.com/dbplot) provides helper functions that automate the aggregation and plotting steps.  `dbplot` is an alternative visualization approach to assist in the best practices of _transforming_ data in the database, then _plotting_ in R.  Please also see this fuller [discussion on creating visualizations](https://db.rstudio.com/best-practices/visualization/) from databases using R.

```{r}
jhu_covid19_durham_sinceApril %>% 
  arrange(date) %>% 
  mutate(daily_count = deaths - lag(deaths)) %>% 
  filter(daily_count > -1) %>% 
  mutate(scare = case_when(
    daily_count >= 2 ~ "high",
    daily_count == 1 ~ "medium",
    daily_count == 0 ~ "low"
  )) %>% 
  ggplot(aes(date, daily_count, "low", "medium")) +
  geom_jitter(aes(color = fct_relevel(scare, "low", "medium"))) +
  geom_smooth(method = lm, se = FALSE) +
  geom_smooth(color = "red", se = FALSE) +
  scale_color_manual(values = c("forestgreen", "goldenrod", "firebrick")) +
  ylim(0, 5) +
  guides(color = FALSE) +
  labs(title = "Covid Deaths", subtitle = "daily count",
       y = "", x = "") +
  theme_classic()
```



```{r}
jhu_covid19_durham_sinceApril %>% 
  arrange(date) %>% 
  mutate(daily_count = confirmed - lag(confirmed)) %>% 
  mutate(scare = case_when(
    daily_count > 59 ~ "extreme",
    daily_count <= 59 & daily_count > 36  ~ "high",
    daily_count <= 36 & daily_count > 19 ~ "medium",
    daily_count <= 19 ~ "low"
  )) %>% 
  ggplot(aes(date, daily_count, "low", "medium")) +
  geom_point(aes(color = fct_relevel(scare, "low", "medium", "high", "extreme"))) +
  geom_smooth(method = lm, se = FALSE) +
  geom_smooth(color = "red", se = FALSE) +
  scale_color_manual(values = c("forestgreen", "goldenrod", "darkorange", "firebrick")) +
  scale_x_date(breaks = "1 month", date_labels = "%b") +
  guides(color = FALSE) +
  labs(title = "COVID19 Cases in Durham County, NC", 
       x = "", y = "daily county",
       caption = "Source: JHU dataset") +
  theme_classic()
```

```{r}
jhu_covid19_DBP_nc_counties %>% 
  select(admin2, fips, province_state, confirmed, deaths) %>% 
  rename(county = admin2, state = province_state, cases = confirmed) %>% 
  arrange(-deaths)
```

## SQL -- MORE

```{sql connection=con, output.var = "special_df"}

SELECT `province_state`, `country_region`, `date`, 
       `latitude`, `longitude`, `confirmed`, `deaths`, 
       `recovered`, `active`, `fips`, `admin2`, `combined_key`
FROM `summary`
WHERE ((`province_state` = 'North Carolina') AND (`date` = '2020-06-30'))
```

Above code chunk created the tibble `special_df` as identified in output.var

```{r}
special_df %>% 
  ggplot(aes(confirmed, deaths)) +
  geom_jitter() +
  geom_text(data = . %>% filter(deaths > 100), 
            aes(confirmed, deaths, label = admin2, hjust = 1.15))
```

## Example 2a: DBplyr and Austin Bikeshare trips

Make the connection to the `bigquery-public-data.austin_bikeshare` database.

```{r}
con2_bq_bikes <- dbConnect(
  bigrquery::bigquery(),
  project = "bigquery-public-data",
  dataset = "austin_bikeshare",
  billing = "workshop-rfun-2021-spring"
)
con2_bq_bikes
```

Set the query pointer to `bigquery-public-data.austin_bikeshare.bikeshare_trips` table.

```{r}
bikeshare_trips <- tbl(con2_bq_bikes, "bikeshare_trips") 
```


```{r}
austin_bikeshare_all <- bikeshare_trips %>% 
  mutate(trip_id = as.double(trip_id)) %>% 
  collect()
```


```{r}
austin_bikeshare_all
```


```{r}
glimpse(austin_bikeshare_all)
```

### Visualize

[How far is too far to bike to work?](https://mobilitylab.org/2017/02/27/how-far-bike-work/)


```{r}
austin_bikeshare_all %>% 
  drop_na(subscriber_type) %>% 
  filter(duration_minutes < 100) %>%
  mutate(travel_type = case_when(
    duration_minutes <= 10 ~ "Commuter",
    duration_minutes >  10 ~ "Tourist"
  )) %>% 
  ggplot(aes(duration_minutes, fct_reorder(fct_lump(subscriber_type, prop = 0.01), duration_minutes))) +
  geom_boxplot(aes(fill = travel_type)) +
  geom_vline(xintercept = 10, linetype = "dashed", color = "grey60") +
  scale_x_log10() +
  labs(title = "Bike Share Trip times", 
       subtitle = "Austin BikeShare Program",
       y = "Type of bike rental pass",
       x = "Duration of ride in minutes",
       caption = "Source: Public datasets > Google BigQuery > Austin Bikeshare Trips",
       fill = "") 
```

```{r}
summary(austin_bikeshare_all)
```

## Example 2b: Austin BikeShare

List the top 5 percent of locations (stations) where bike-share trips begin and end.

Then, use `left_join()` to affect a database SQL join or table merge.

```{r}
left_tbl <- bikeshare_trips %>% 
  count(start_station_name) %>% 
  slice_max(order_by = n, prop = .05)
left_tbl

right_tbl <- bikeshare_trips %>% 
  count(end_station_name) %>% 
  slice_max(order_by = n, prop = .05)
right_tbl

left_join(left_tbl, right_tbl, by = c("start_station_name" = "end_station_name")) # %>% show_query()
```

## Example 3: NYC tree census

Inspired by [Edgar Ruiz's dbplyr presentation](https://rstudio.com/resources/rstudioconf-2019/databases-using-r-the-latest/)

[DBplot](https://db.rstudio.com/dbplot/) leverages dplyr to process the calculations of a plot inside a database.

```{r}
con_ny_trees <- dbConnect(
  bigrquery::bigquery(),
  project = "bigquery-public-data",
  dataset = "new_york_trees",
  billing = "workshop-rfun-2021-spring"
)
con_ny_trees
```

Set the query pointer to `bigquery-public-data.ustin_bikeshare.bikesahre_trips` table

```{r}
nytrees <- tbl(con_ny_trees, "tree_census_2015") 
```

### Visualize


```{r}
library(dbplot)
library(leaflet)
```


```{r}
glimpse(nytrees)
nytrees %>% 
  count(curb_loc)
```

#### dbplot_raster()

Above, we **subset the data at the database** to _good and healthy trees_, _offset from the curb_.  Now use the `dbpot_raster()` function to **plot in R**

```{r}
locations <- nytrees %>% 
  filter(curb_loc == "OffsetFromCurb") %>% 
  filter(health == "Good") %>% 
  dbplot_raster(longitude, latitude, resolution = 30)
  # db_compute_raster(longitude, latitude, resolution = 30)
locations
```
#### db_compute_raster()

```{r}
nytrees %>% 
  filter(curb_loc == "OffsetFromCurb") %>% 
  filter(health == "Good") %>% 
  count()

locations_tbl <- nytrees %>% 
  filter(curb_loc == "OffsetFromCurb") %>% 
  filter(health == "Good") %>% 
  db_compute_raster(longitude, latitude, resolution = 30)
locations_tbl
```

#### Tidy evaluation functions

Tidy eval function [by Ruiz (timestamp 13:47)](https://rstudio.com/resources/rstudioconf-2019/databases-using-r-the-latest/)

```{r}
size <- function(df, field) {
  field <- enquo(field)
  df %>% 
    arrange(!! field) %>% 
    mutate(diff = !! field - lag(!! field)) %>% 
    filter(diff > 0) %>% 
    summarise(min(diff)) %>% 
    pull()
}
```


```{r}
lon_size <- locations_tbl %>% 
  size(longitude)

lon_size

lat_size <- locations_tbl %>% 
  size(latitude)

lat_size
```



```{r}
sq <- locations_tbl %>% 
  mutate(lon1 = longitude,
         lon2 = longitude + lon_size,
         lat1 = latitude,
         lat2 = latitude + lat_size,
         of_max = `n()` / max(`n()`)
  )
```

#### Leaflet map

```{r}
leaflet() %>% 
  addTiles() %>% 
  addRectangles(
    sq$lon1, sq$lat1, sq$lon2, sq$lat2
  )
```


```{r}
fancy <- leaflet() %>% 
  addTiles() %>% 
  addProviderTiles(providers$CartoDB.Positron) %>% 
  addRectangles(
    sq$lon1, sq$lat1, sq$lon2, sq$lat2,
    fillOpacity = sq$of_max,
    fillColor = "forestgreen",
    stroke = FALSE,
    popup = glue::glue('Trees {sq$`n()`}')
  )

fancy
```


```{r}
library(mapview)

mapshot(fancy, file = "images/tree_cover.png")
```


## Resources

- [Databases using R](https://db.rstudio.com/)
- [library(DBI)](https://dbi.r-dbi.org/reference/)
- [library(bigrquery)](https://bigrquery.r-dbi.org/)
- [library(dbplyr)](https://dbplyr.tidyverse.org/)
- [RStudio Conf 2019, 15 min. Recording](https://rstudio.com/resources/rstudioconf-2019/databases-using-r-the-latest/)
