---
title: "Connecting to Google BigQuery with DBplyr"
output: html_notebook
---

## Google Cloud Account & Big Query

1) Get a Google Cloud account

    - [Google Cloud Free Program](https://cloud.google.com/free/docs/gcp-free-tier)
    - [BigQuery public datasets](https://cloud.google.com/bigquery/public-data)
    - [BigQuery](https://cloud.google.com/bigquery)
    - [GCP-BigQuery Console](https://console.cloud.google.com/bigquery)



Following [Kevin Wang's document](https://kevinwang09.github.io/post/bigquery-in-r/)

Gonna search JHU's Covid19 public database


## Library packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(DBI) 
library(bigrquery)
```



The `dbConnect()` argument, `billing`, should have the value of a GCP project ID

GPC > Home > Dashboard
GPC > BigQuery

```{r}
# library(DBI)

con <- dbConnect(
  bigquery(),
  project = "bigquery-public-data",
  dataset = "covid19_jhu_csse",
  billing = "workshop-rfun-2021-spring"
)
con
```

Now, from **within the R console**, authenticate with GCP

```
bigrquery::bq_auth()
```

### DBI apprroach

```
DBI::dbGetQuery()
```

## Pointer to the a BQ database

Let's search the JHU COVID19 summary table


```{r}
my_db_pointer <- tbl(con, "summary")
glimpse(my_db_pointer)
tally(my_db_pointer)
```



## SQL

Using `DBI`, can compose SQL directly.

```
{r}
DBI::dbGetQuery(con, 
"SELECT *
FROM `bigquery-public-data.covid19_jhu_csse.summary`
LIMIT 10;")
```

```
{r}
jhu_covid19_nc_counties_10 <- dbGetQuery(con,
           "SELECT
              admin2, province_state, confirmed
            FROM
              `bigquery-public-data.covid19_jhu_csse.summary` 
            WHERE 
              country_region = 'US'
              AND date = '2020-06-30'
              AND province_state = 'North Carolina'
            LIMIT 10;"
)
jhu_covid19_nc_counties_10
```


```
{r}
jhu_covid19_nc_counties <- dbGetQuery(con,
           "SELECT
              admin2, province_state, confirmed
            FROM
              `bigquery-public-data.covid19_jhu_csse.summary` 
            WHERE 
              country_region = 'US'
              AND date = '2020-06-30'
              AND province_state = 'North Carolina'"
)
jhu_covid19_nc_counties
```

### DBplyr approach

If you don't know SQL, or are more fluent in `dplyr`, you can compose your queries with `dbplyr`.

First make a connection....

```{r}
covid_data_DBplyr_style <- tbl(con, "summary") 
```

an SQL query BUT using `dplyr` syntax

> Note: `collect()` will run a query that has been assigned to an object

```{r}
jhu_covid19_DBP_nc_counties <- covid_data_DBplyr_style %>% 
  filter(province_state == "North Carolina", date == '2020-06-30')  %>% 
  select(province_state, country_region, date, latitude, 
         longitude, confirmed, deaths, #location_geom,
         recovered, active, fips, admin2, combined_key) %>% 
  # show_query()
  collect()

jhu_covid19_DBP_nc_counties
```

```{sql connection=con, output.var = "special_df"}

SELECT `province_state`, `country_region`, `date`, 
       `latitude`, `longitude`, `confirmed`, `deaths`, 
       `recovered`, `active`, `fips`, `admin2`, `combined_key`
FROM `summary`
WHERE ((`province_state` = 'North Carolina') AND (`date` = '2020-06-30'))
```

```{r}
special_df %>% 
  ggplot(aes(confirmed, deaths)) +
  geom_jitter() +
  geom_text(data = . %>% filter(deaths > 100), 
            aes(confirmed, deaths, label = admin2, hjust = 1.15))
```




```{r}
glimpse(my_db_pointer)

my_search <- my_db_pointer %>% 
  filter(province_state == "North Carolina", date == '2020-06-30')  %>% 
  select(province_state, country_region, date, latitude, 
         longitude, confirmed, deaths, #location_geom,
         recovered, active, fips, admin2, combined_key)

my_search  

my_local_df <- my_search %>% 
  collect()
my_local_df

my_db_pointer %>% 
  filter(province_state == "North Carolina", date == '2020-06-30')  %>% 
  select(province_state, country_region, date, latitude, 
         longitude, confirmed, deaths, #location_geom,
         recovered, active, fips, admin2, combined_key)
```



```{r}
jhu_covid19_durham_sinceApril <- covid_data_DBplyr_style %>% 
  filter(province_state == "North Carolina", 
         admin2 == "Durham",
         date >= '2020-04-01')  %>% 
  select(date, confirmed, deaths, recovered, active, 
         fips, admin2, combined_key) %>% 
  collect()

jhu_covid19_durham_sinceApril
```


```{r}
jhu_covid19_durham_sinceApril %>% 
  arrange(date)
```

```{r}
jhu_covid19_durham_sinceApril %>% 
  arrange(date) %>% 
  mutate(daily_count = deaths - lag(deaths)) %>% 
  filter(daily_count > -1) %>% 
  mutate(scare = case_when(
    daily_count >= 2 ~ "high",
    daily_count == 1 ~ "medium",
    daily_count == 0 ~ "low"
  )) %>% 
  ggplot(aes(date, daily_count, "low", "medium")) +
  # geom_line() +
  geom_jitter(aes(color = fct_relevel(scare, "low", "medium"))) +
  # geom_density() +
  # geom_area() +
  geom_smooth(method = lm, se = FALSE) +
  geom_smooth(color = "red", se = FALSE) +
  scale_color_manual(values = c("forestgreen", "goldenrod", "firebrick")) +
  ylim(0, 5) +
  guides(color = FALSE) +
  labs(title = "Covid Deaths", subtitle = "daily count",
       y = "", x = "") +
  theme_classic()
```



```{r}
jhu_covid19_durham_sinceApril %>% 
  arrange(date) %>% 
  mutate(daily_count = confirmed - lag(confirmed)) %>% 
  mutate(scare = case_when(
    daily_count > 59 ~ "extreme",
    daily_count <= 59 & daily_count > 36  ~ "high",
    daily_count <= 36 & daily_count > 19 ~ "medium",
    daily_count <= 19 ~ "low"
  )) %>% 
  ggplot(aes(date, daily_count, "low", "medium")) +
  # geom_line() +
  geom_point(aes(color = fct_relevel(scare, "low", "medium", "high", "extreme"))) +
  geom_smooth(method = lm, se = FALSE) +
  geom_smooth(color = "red", se = FALSE) +
  scale_color_manual(values = c("forestgreen", "goldenrod", "darkorange", "firebrick")) +
  # scale_color_brewer(palette = "YlOrRd") +
  scale_x_date(breaks = "1 month", date_labels = "%b") +
  # scale_x_continuous(breaks = seq(1,182, by = 30),
  #                    labels = month.abb[4:10]) +
  guides(color = FALSE) +
  labs(title = "COVID19 Cases in Durham County, NC", 
       x = "", y = "daily county",
       caption = "Source: JHU dataset") +
  theme_classic()
```

```{r}
jhu_covid19_DBP_nc_counties %>% 
  select(admin2, fips, province_state, confirmed, deaths) %>% 
  rename(county = admin2, state = province_state, cases = confirmed) %>% 
  arrange(-deaths)
```

## DBplyr and Austin Bikeshare trips


```{r}
con2_bq_bikes <- dbConnect(
  bigrquery::bigquery(),
  project = "bigquery-public-data",
  dataset = "austin_bikeshare",
  billing = "infra-radius-291118"
)
con2_bq_bikes
```


```{r}
bikeshare_trips <- tbl(con2_bq_bikes, "bikeshare_trips") 
```


```{r}
austin_bikeshare_trips <- bikeshare_trips %>% 
  mutate(trip_id = as.character(trip_id)) %>% 
  head(100000) %>%
  collect()
```

```{r}
range(austin_bikeshare_trips$start_time)
```

date range of retrieved data:   "2013-12-21 10:12:00 UTC" "2020-06-30 21:27:29 UTC"

```{r}
austin_bikeshare_trips %>% 
  arrange(start_time)
```

### compare

I was having some trouble with bulk SQL GET via big-query, so I  selected all at the console and saved to google drive (then download, now import).  Comparing the tables....

in bq_console:

```
SELECT COUNT(*)
FROM `bigquery-public-data.austin_bikeshare.bikeshare_trips`
```

ANSWER:  1,264,113


```
{r}
austin_trips <- read_csv("data/austin_bike_trips.csv")

austin_trips
```


So, I'm not sure what the error is about.  Maybe if I do a DBI query it will work?

```
{r}
austin_trips
```
```
{r}
range(austin_trips$start_time)
```

```
{r}
glimpse(austin_trips)
```



```
{r}
austen_viaDBI <- dbGetQuery(con2_bq_bikes,
           "SELECT
              *
            FROM
              `bigquery-public-data.austin_bikeshare.bikeshare_trips`
              "
            )
```

**Error:**
```
Running job 'infra-radius-291118.job_piWh-nbTwTkc37iHiqVpCUOubawc.US' [\] 14s
Complete
Billed: 136.31 MB
Downloading 1,264,113 rows in 127 pages.
Downloading data [===>------------------------------------------------------------------------------]   5% ETA: 42sError: Invalid value at 'start_index' (TYPE_UINT64), "1e+05" [invalid] 

```

### Date comparison

- from dbplyr:

    -  2013-12-21 10:12:00 UTC" "2020-06-30 21:27:29 UTC"

- from bq_console_download_ingest:

    - "2013-12-21 09:12:00 UTC" "2020-06-30 22:04:11 UTC"
    
    
So there is some scrunchy data in there somewhere.  Or a problem with my SQL approach.  Anyway, the purpose of `DBplyr` and `DBI` is not about downloading the whole table.  It's about gathering just the data that is needed. 


```
{r}
austin_trips %>% 
  count(subscriber_type, sort = TRUE)
```


```{r}
austin_bikeshare_trips %>% 
  count(subscriber_type, sort = TRUE) %>% 
  filter(str_detect(subscriber_type, "eekender")) #regex("weekender", ignore_case = TRUE)))
```

## Error / `options(scipen = 20)`

At some point, I'm not able to download all the data.  I get the error:  

> Invalid value at 'start_index' (TYPE_UINT64), "1e+05" [invalid]

[according to stackoverflow.com](https://stackoverflow.com/questions/63328348/invalid-value-at-start-index-type-uint64-1e05-invalid-issue-while-down)

_The issue is caused as BigQuery allows only 100k records to be downloaded. _


_Adding the `options(scipen = 20)` script to the start of your code will solve the issue._

Or, updating the `bigrquery` package to the latest version will fix your issue.


```{r}
# options(scipen = 20)
```



```{r}
austin_bikeshare_walkup <- bikeshare_trips %>% 
  mutate(trip_id = as.double(trip_id)) %>% 
  # select(trip_id:duration_minutes) %>% 
  filter(subscriber_type == "Walk Up") %>% 
  #        start_time <= "2016-01-01") %>% 
  # head(100000) %>%
  collect()
```


```{r}
austin_bikeshare_all <- bikeshare_trips %>% 
  mutate(trip_id = as.double(trip_id)) %>% 
  # filter(subscriber_type == "Walk Up") %>% 
  collect()
```


```{r}
austin_bikeshare_all
#austin_trips
```

```
{r}
all_equal(austin_bikeshare_all, austin_trips)
```

```{r}
bigrquery::bq_table_fields("bigquery-public-data.austin_bikeshare.bikeshare_trips")
```



```{r}
glimpse(austin_bikeshare_all)
# print("\n-----------------\n")
# glimpse(austin_trips)
```

[How far is to far to bike to work?](https://mobilitylab.org/2017/02/27/how-far-bike-work/)


```{r}
austin_bikeshare_all %>% 
  drop_na(subscriber_type) %>% 
  filter(duration_minutes < 100) %>%
  mutate(travel_type = case_when(
    duration_minutes <= 10 ~ "Commuter",
    duration_minutes >  10 ~ "Tourist"
  )) %>% 
  # filter(subscriber_type == "Walk Up") %>% 
  ggplot(aes(duration_minutes, fct_reorder(fct_lump(subscriber_type, prop = 0.01), duration_minutes))) +
  geom_boxplot(aes(fill = travel_type)) +
  geom_vline(xintercept = 10, linetype = "dashed", color = "grey60") +
  scale_x_log10() +
  # facet_wrap(~ travel_type) +
  # guides(fill = FALSE)
  labs(title = "Bike Share Trip times", 
       subtitle = "Austin BikeShare Program",
       y = "Type of bike rental pass",
       x = "Duration of ride in minutes",
       caption = "Source: Public datasets > Google BigQuery > Austin Bikeshare Trips",
       fill = "") 
  # facet_wrap(~ fct_reorder(fct_lump(subscriber_type, prop = 0.01), duration_minutes))
```

```{r}
summary(austin_bikeshare_all)
```


## Resources

- [Databases using R](https://db.rstudio.com/)
- [library(DBI)](https://dbi.r-dbi.org/reference/)
- [library(bigrquery)](https://bigrquery.r-dbi.org/)
- [library(dbplyr)](https://dbplyr.tidyverse.org/)
- [RStudio Conf 2019, 15 min. Recording](https://rstudio.com/resources/rstudioconf-2019/databases-using-r-the-latest/)
